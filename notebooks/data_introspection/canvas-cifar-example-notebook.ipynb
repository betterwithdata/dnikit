{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f25846",
   "metadata": {},
   "source": [
    "# Symphony: CIFAR-10 Example\n",
    "\n",
    "**Visualizing a [DNIKit](https://betterwithdata.github.io/dnikit/) [Dataset Report](https://betterwithdata.github.io/dnikit/introspectors/data_introspection/dataset_report.html)**\n",
    "\n",
    "This is an example of visualizing the CIFAR-10 dataset with Symphony. Beyond the image samples themselves, we've used [DNIKit](https://betterwithdata.github.io/dnikit/) to compute some other statistics about the data. Symphony uses this data in the Familiarity and Duplicates widgets.\n",
    "\n",
    "In DNIKit, you can create a `DatasetReport` object, that has a `data` field, which is a pandas DataFrame table with metadata about each data sample like its familiarity, duplicates, overall summary, and dimensionality projection coordinates. Symphony can directly visualize this DataFrame.\n",
    "\n",
    "For this example, we'll load a precomputed analysis for the CIFAR-10 dataset that has been saved to disk as a pandas DataFrame. If you are interested in generating this DataFrame yourself (or for a different dataset or model), see [this DNIKit example](https://betterwithdata.github.io/dnikit/notebooks/data_introspection/dataset_report.ipynb). This Symphony example picks up at the end of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21f976",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Symphony in Jupyter Notebooks\n",
    "\n",
    "Let's use Symphony to explore this dataset in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d904ddac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 12:09:48.288161: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-25 12:09:48.312658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 12:09:48.694747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5882f87-25ef-4b37-8906-4f6098f57950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy     : 1.26.4\n",
      "scipy     : not installed\n",
      "traitlets : 5.14.3\n",
      "tqdm      : not installed\n",
      "easyimages: not installed\n",
      "tensorflow: 2.16.1\n",
      "keras     : 3.4.1\n",
      "dnikit    : not installed\n",
      "cffi      : 1.16.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark(packages=\"numpy,scipy,traitlets,tqdm,easyimages,tensorflow,keras,dnikit,cffi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df4b17",
   "metadata": {},
   "source": [
    "Let's first load and download the CIFAR-10 dataset. We'll save it to a folder named `cifar`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f60f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./cifar/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2d4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "class_to_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Concatenate the train and test into one array, as well as the train/test labels, and the class labels\n",
    "full_dataset = np.concatenate((x_train, x_test))\n",
    "dataset_labels = ['train']*len(x_train) + ['test']*len(x_test)\n",
    "class_labels = np.squeeze(np.concatenate((y_train, y_test)))\n",
    "\n",
    "# Helper function for file pathing\n",
    "def class_path(index, dataset_labels, class_labels):\n",
    "    return f\"{dataset_labels[index]}/{class_to_name[int(class_labels[index])]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through data and save images to `cifar` folder\n",
    "for idx in range(full_dataset.shape[0]):\n",
    "    base_path = os.path.join(data_path, class_path(idx, dataset_labels, class_labels))\n",
    "    Path(base_path).mkdir(exist_ok=True, parents=True)\n",
    "    filename = os.path.join(base_path, f\"image{idx}.png\")\n",
    "    # Write to disk after converting to BGR format, used by opencv\n",
    "    cv2.imwrite(filename, cv2.cvtColor(full_dataset[idx, ...], cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef06c2",
   "metadata": {},
   "source": [
    "Now that we have the images saved, we can load our precomputed analysis from DNIKit to visualize in Symphony. You can use Symphony to visualize CIFAR-10, and other datsets, directly. But some components require special metadata that we can use DNIKit's Dataset Report to generate automatically for us.\n",
    "\n",
    "We can also print out the DataFrame to see the types of metadata columns that are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('symphony_cifar_example.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14f298-482f-4485-9210-1774086e47f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a5104",
   "metadata": {},
   "source": [
    "To use Symphony, we'll import the main library and instantiate a Symphony object, passing the pandas DataFrame analysis and a file path to the dataset we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf60bd1-6e0f-46db-af5f-8331a8812786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfb432-4f0e-48b0-adeb-8fb3b3d303b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table.slice(0, 1)['id'].to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f2dff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import symphony_ux\n",
    "\n",
    "symph = symphony_ux.Symphony(df, files_path=str(data_path), notebook = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fedddc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To use the different Symphony widgets, you can import them indepdently. Let's first look at the Summary widget to see the overall distributions of our datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0eebc6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symphony spect dict value is {'width': 'XXL', 'height': 'M', 'page': 'Summary', 'name': 'SymphonySummary', 'description': 'A Symphony component that visualizes an overview of a dataset', 'summaryElements': []}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6705b28d76a4646a28c7da519aea05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonySummary(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL', 'he…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_summary import SymphonySummary\n",
    "\n",
    "symph.widget(SymphonySummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d3ad1",
   "metadata": {},
   "source": [
    "Instead of a summary, if we want to browse through the data we can use the List widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0cc1ce2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symphony spect dict value is {'width': 'XXL', 'height': 'M', 'page': 'List', 'name': 'SymphonyList', 'description': 'A Symphony component that displays a view of data instances'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16be946a5e2744cb9285205a963fff05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonyList(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL', 'heigh…"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_list import SymphonyList\n",
    "\n",
    "symph.widget(SymphonyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2877bd-43fa-4176-a9d0-ab1ae0f05470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d7ab972",
   "metadata": {},
   "source": [
    "It's common to use dimensionality reduction techniques to summarize and find patterns in ML dataset. DNIKit already ran a reduction, and saves it when running a DataSet Report. We can use the Scatterplot widget to visualize this embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f188e4f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb5bd369a5c4695b25f5b3c76d8f039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonyScatterplot(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL',…"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_scatterplot import SymphonyScatterplot\n",
    "\n",
    "symph.widget(SymphonyScatterplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a918cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some datasets can contain duplicates: data instances that are the same or very similar to others. These can be hard to find, and become espeically problematic if the same data instance is in the training and testing splits. We can answer these questions using the Duplicates widget.\n",
    "\n",
    "Hint: Take a look at the `automobile` class, where there are duplicates across train and test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99eb410",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from symphony_duplicates import SymphonyDuplicates\n",
    "\n",
    "symph.widget(SymphonyDuplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7579f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, we can use advanced ML metrics and the Familiarity widget to find the most and least representative data instances from a given datset, which can help identify model biases and annotation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aaa6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from symphony_familiarity import SymphonyFamiliarity\n",
    "\n",
    "symph.widget(SymphonyFamiliarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe645eb9",
   "metadata": {},
   "source": [
    "## Symphony as a Standalone Export\n",
    "\n",
    "Symphony can also be exported as a standalone static export to be shared with others or hosted. To explore this example in a web browser, you can export the report to local folder.\n",
    "\n",
    "If you only want to visualize locally without sharing the data, you can specify Symphony to handle the paths for a local standlone visualization by setting ``symlink_files`` to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "symph.export('./symphony_report', name=\"Symphony CIFAR-10 Example\", symlink_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710755c2",
   "metadata": {},
   "source": [
    "You can now serve the dataset report. For example, from the `symphony_export` folder, run a simple server from the command line:\n",
    "\n",
    "```bash\n",
    "python -m http.server\n",
    "```\n",
    "\n",
    "And navigate to http://localhost:8000/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faff5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1638994625349,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "betterwithdata",
   "language": "python",
   "name": "betterwithdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
