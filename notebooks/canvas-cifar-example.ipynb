{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f25846",
   "metadata": {},
   "source": [
    "# Canvas: CIFAR-10 Example\n",
    "\n",
    "**Visualizing a [DNIKit](https://betterwithdata.github.io/dnikit/) [Dataset Report](https://betterwithdata.github.io/dnikit/introspectors/data_introspection/dataset_report.html)**\n",
    "\n",
    "This is an example of visualizing the CIFAR-10 dataset with Symphony. Beyond the image samples themselves, we've used [DNIKit](https://betterwithdata.github.io/dnikit/) to compute some other statistics about the data. Symphony uses this data in the Familiarity and Duplicates widgets.\n",
    "\n",
    "In DNIKit, you can create a `DatasetReport` object, that has a `data` field, which is a pandas DataFrame table with metadata about each data sample like its familiarity, duplicates, overall summary, and dimensionality projection coordinates. Symphony can directly visualize this DataFrame.\n",
    "\n",
    "For this example, we'll load a precomputed analysis for the CIFAR-10 dataset that has been saved to disk as a pandas DataFrame. If you are interested in generating this DataFrame yourself (or for a different dataset or model), see [this DNIKit example](https://betterwithdata.github.io/dnikit/notebooks/data_introspection/dataset_report.ipynb). This Symphony example picks up at the end of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21f976",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Symphony in Jupyter Notebooks\n",
    "\n",
    "Let's use Symphony to explore this dataset in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d904ddac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 00:12:07.975538: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-29 00:12:07.995549: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-29 00:12:07.995567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-29 00:12:07.996246: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 00:12:08.000066: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-29 00:12:08.372515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532d28f9-c27f-4010-bdca-d7b7a3238f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/satish/Development/workspace/projects/deepview_dev/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5882f87-25ef-4b37-8906-4f6098f57950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy     : 1.26.4\n",
      "scipy     : 1.14.1\n",
      "traitlets : 5.14.3\n",
      "tqdm      : 4.67.1\n",
      "easyimages: not installed\n",
      "tensorflow: 2.15.0\n",
      "keras     : 2.15.0\n",
      "deepview  : 2.0.1\n",
      "cffi      : 1.17.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark(packages=\"numpy,scipy,traitlets,tqdm,easyimages,tensorflow,keras,deepview,cffi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df4b17",
   "metadata": {},
   "source": [
    "Let's first load and download the CIFAR-10 dataset. We'll save it to a folder named `cifar`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f60f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"./cifar/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b2d4a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "class_to_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Concatenate the train and test into one array, as well as the train/test labels, and the class labels\n",
    "full_dataset = np.concatenate((x_train, x_test))\n",
    "dataset_labels = ['train']*len(x_train) + ['test']*len(x_test)\n",
    "class_labels = np.squeeze(np.concatenate((y_train, y_test)))\n",
    "\n",
    "# Helper function for file pathing\n",
    "def class_path(index, dataset_labels, class_labels):\n",
    "    return f\"{dataset_labels[index]}/{class_to_name[int(class_labels[index])]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3853dc-ebf1-44aa-85a1-f9bf8d79b704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e131175-0a15-4c45-83ed-b490a67c3a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pickle\\n\\nwith open('dnikit_with_image_data.pickle', 'wb') as handle:\\n    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pickle\n",
    "\n",
    "with open('dnikit_with_image_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253c425e-b2dc-4490-acaf-fe14fc2d511a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee9d9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Loop through data and save images to `cifar` folder\\nfrom tqdm.notebook import tqdm\\nrawimage_data = {}\\nfor idx in tqdm(range(full_dataset.shape[0])):\\n    base_path = os.path.join(data_path, class_path(idx, dataset_labels, class_labels))\\n    Path(base_path).mkdir(exist_ok=True, parents=True)\\n    filename = os.path.join(base_path, f\"image{idx}.png\")\\n    # Write to disk after converting to BGR format, used by opencv\\n    cv2.imwrite(filename, cv2.cvtColor(full_dataset[idx, ...], cv2.COLOR_RGB2BGR))\\n    rawimage_data[os.path.join(class_path(idx, dataset_labels, class_labels), f\"image{idx}.png\")] = encode_image(filename)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Loop through data and save images to `cifar` folder\n",
    "from tqdm.notebook import tqdm\n",
    "rawimage_data = {}\n",
    "for idx in tqdm(range(full_dataset.shape[0])):\n",
    "    base_path = os.path.join(data_path, class_path(idx, dataset_labels, class_labels))\n",
    "    Path(base_path).mkdir(exist_ok=True, parents=True)\n",
    "    filename = os.path.join(base_path, f\"image{idx}.png\")\n",
    "    # Write to disk after converting to BGR format, used by opencv\n",
    "    cv2.imwrite(filename, cv2.cvtColor(full_dataset[idx, ...], cv2.COLOR_RGB2BGR))\n",
    "    rawimage_data[os.path.join(class_path(idx, dataset_labels, class_labels), f\"image{idx}.png\")] = encode_image(filename)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef06c2",
   "metadata": {},
   "source": [
    "Now that we have the images saved, we can load our precomputed analysis from DNIKit to visualize in Symphony. You can use Symphony to visualize CIFAR-10, and other datsets, directly. But some components require special metadata that we can use DNIKit's Dataset Report to generate automatically for us.\n",
    "\n",
    "We can also print out the DataFrame to see the types of metadata columns that are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e3021a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('canvas_cifar_example.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ce09e3-cd87-4e6d-871e-de1ad3b3a022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = pd.read_pickle('dnikit_with_image_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d14f298-482f-4485-9210-1774086e47f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   id                                          60000 non-null  object \n",
      " 1   class                                       60000 non-null  object \n",
      " 2   dataset                                     60000 non-null  object \n",
      " 3   duplicates_conv_pw_13                       60000 non-null  int32  \n",
      " 4   projection_conv_pw_13_x                     60000 non-null  float32\n",
      " 5   projection_conv_pw_13_y                     60000 non-null  float32\n",
      " 6   familiarity_conv_pw_13                      60000 non-null  float64\n",
      " 7   splitFamiliarity_conv_pw_13_byAttr_class    60000 non-null  object \n",
      " 8   splitFamiliarity_conv_pw_13_byAttr_dataset  60000 non-null  object \n",
      "dtypes: float32(2), float64(1), int32(1), object(5)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f9c8f7-1123-479d-bd5e-c91a88ed5bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/satish/Development/workspace/projects/deepview_dev/notebooks'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10bf770d-4125-4715-b917-f8d6d9e6ee19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>dataset</th>\n",
       "      <th>duplicates_conv_pw_13</th>\n",
       "      <th>projection_conv_pw_13_x</th>\n",
       "      <th>projection_conv_pw_13_y</th>\n",
       "      <th>familiarity_conv_pw_13</th>\n",
       "      <th>splitFamiliarity_conv_pw_13_byAttr_class</th>\n",
       "      <th>splitFamiliarity_conv_pw_13_byAttr_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/frog/image0.png</td>\n",
       "      <td>frog</td>\n",
       "      <td>train</td>\n",
       "      <td>-1</td>\n",
       "      <td>8.996793</td>\n",
       "      <td>6.067759</td>\n",
       "      <td>-63.839685</td>\n",
       "      <td>{'frog': -57.4634189968481, 'truck': -79.73388...</td>\n",
       "      <td>{'train': -63.85114012943654, 'test': -63.7891...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/truck/image1.png</td>\n",
       "      <td>truck</td>\n",
       "      <td>train</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.987265</td>\n",
       "      <td>4.231816</td>\n",
       "      <td>-56.638484</td>\n",
       "      <td>{'frog': -63.61311723243215, 'truck': -52.5701...</td>\n",
       "      <td>{'train': -56.667391185887446, 'test': -56.511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/truck/image2.png</td>\n",
       "      <td>truck</td>\n",
       "      <td>train</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.857024</td>\n",
       "      <td>5.526045</td>\n",
       "      <td>-63.064532</td>\n",
       "      <td>{'frog': -66.21848904767097, 'truck': -61.3058...</td>\n",
       "      <td>{'train': -63.04006769721045, 'test': -63.1636...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/deer/image3.png</td>\n",
       "      <td>deer</td>\n",
       "      <td>train</td>\n",
       "      <td>-1</td>\n",
       "      <td>11.347506</td>\n",
       "      <td>4.376108</td>\n",
       "      <td>-70.230302</td>\n",
       "      <td>{'frog': -77.75308621492171, 'truck': -87.4106...</td>\n",
       "      <td>{'train': -70.17086035521375, 'test': -70.4808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/automobile/image4.png</td>\n",
       "      <td>automobile</td>\n",
       "      <td>train</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.506487</td>\n",
       "      <td>2.882784</td>\n",
       "      <td>-73.770091</td>\n",
       "      <td>{'frog': -85.30263297363926, 'truck': -82.4036...</td>\n",
       "      <td>{'train': -73.6560069777013, 'test': -74.43546...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id       class dataset  duplicates_conv_pw_13  \\\n",
       "0        train/frog/image0.png        frog   train                     -1   \n",
       "1       train/truck/image1.png       truck   train                     -1   \n",
       "2       train/truck/image2.png       truck   train                     -1   \n",
       "3        train/deer/image3.png        deer   train                     -1   \n",
       "4  train/automobile/image4.png  automobile   train                     -1   \n",
       "\n",
       "   projection_conv_pw_13_x  projection_conv_pw_13_y  familiarity_conv_pw_13  \\\n",
       "0                 8.996793                 6.067759              -63.839685   \n",
       "1                 0.987265                 4.231816              -56.638484   \n",
       "2                 3.857024                 5.526045              -63.064532   \n",
       "3                11.347506                 4.376108              -70.230302   \n",
       "4                -1.506487                 2.882784              -73.770091   \n",
       "\n",
       "            splitFamiliarity_conv_pw_13_byAttr_class  \\\n",
       "0  {'frog': -57.4634189968481, 'truck': -79.73388...   \n",
       "1  {'frog': -63.61311723243215, 'truck': -52.5701...   \n",
       "2  {'frog': -66.21848904767097, 'truck': -61.3058...   \n",
       "3  {'frog': -77.75308621492171, 'truck': -87.4106...   \n",
       "4  {'frog': -85.30263297363926, 'truck': -82.4036...   \n",
       "\n",
       "          splitFamiliarity_conv_pw_13_byAttr_dataset  \n",
       "0  {'train': -63.85114012943654, 'test': -63.7891...  \n",
       "1  {'train': -56.667391185887446, 'test': -56.511...  \n",
       "2  {'train': -63.04006769721045, 'test': -63.1636...  \n",
       "3  {'train': -70.17086035521375, 'test': -70.4808...  \n",
       "4  {'train': -73.6560069777013, 'test': -74.43546...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b42436e0-3668-475a-a132-2f9bef8d4a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/satish/miniforge3/envs/betterwithdata/lib/python310.zip', '/home/satish/miniforge3/envs/betterwithdata/lib/python3.10', '/home/satish/miniforge3/envs/betterwithdata/lib/python3.10/lib-dynload', '', '/home/satish/.local/lib/python3.10/site-packages', '/home/satish/miniforge3/envs/betterwithdata/lib/python3.10/site-packages', '/home/satish/miniforge3/envs/betterwithdata/lib/python3.10/site-packages/setuptools/_vendor']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5e2e01-0cc2-418a-8a37-5bd3c56e751d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   id                                          60000 non-null  object \n",
      " 1   class                                       60000 non-null  object \n",
      " 2   dataset                                     60000 non-null  object \n",
      " 3   duplicates_conv_pw_13                       60000 non-null  int32  \n",
      " 4   projection_conv_pw_13_x                     60000 non-null  float32\n",
      " 5   projection_conv_pw_13_y                     60000 non-null  float32\n",
      " 6   familiarity_conv_pw_13                      60000 non-null  float64\n",
      " 7   splitFamiliarity_conv_pw_13_byAttr_class    60000 non-null  object \n",
      " 8   splitFamiliarity_conv_pw_13_byAttr_dataset  60000 non-null  object \n",
      "dtypes: float32(2), float64(1), int32(1), object(5)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a5104",
   "metadata": {},
   "source": [
    "To use Symphony, we'll import the main library and instantiate a Symphony object, passing the pandas DataFrame analysis and a file path to the dataset we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab1f2dff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files path is ././cifar/\n",
      "Canvas spect dict value is {'filesPath': '././cifar/', 'dataType': 2, 'instancesPerPage': 40, 'showUnfilteredData': True, 'idColumn': 'id'}\n"
     ]
    }
   ],
   "source": [
    "import canvas_ux\n",
    "\n",
    "symph = canvas_ux.Canvas(df, files_path=str(data_path), notebook = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fedddc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To use the different Symphony widgets, you can import them indepdently. Let's first look at the Summary widget to see the overall distributions of our datset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc81167-1818-41f7-b4c6-57b74c737a2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./cifar/train/bird/image48.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d3ad1",
   "metadata": {},
   "source": [
    "Instead of a summary, if we want to browse through the data we can use the List widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0cc1ce2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/Development/workspace/projects/ml-symphony-dev/examples\n",
      "Symphony spect dict value is {'width': 'XXL', 'height': 'M', 'page': 'List', 'name': 'SymphonyList', 'description': 'A Symphony component that displays a view of data instances'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985c74d0a5594efaab41a3c11ade8a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonyList(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL', 'heigh…"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_list import SymphonyList\n",
    "\n",
    "symph.widget(SymphonyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0eebc6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symphony spect dict value is {'width': 'XXL', 'height': 'M', 'page': 'Summary', 'name': 'SymphonySummary', 'description': 'A Symphony component that visualizes an overview of a dataset', 'summaryElements': []}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54360a307ec47748bf718c4903a8486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonySummary(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL', 'he…"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_summary import SymphonySummary\n",
    "\n",
    "symph.widget(SymphonySummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2877bd-43fa-4176-a9d0-ab1ae0f05470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d7ab972",
   "metadata": {},
   "source": [
    "It's common to use dimensionality reduction techniques to summarize and find patterns in ML dataset. DNIKit already ran a reduction, and saves it when running a DataSet Report. We can use the Scatterplot widget to visualize this embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f188e4f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symphony spect dict value is {'width': 'XXL', 'height': 'M', 'page': 'Scatterplot', 'name': 'SymphonyScatterplot', 'description': 'A scatterplot Symphony component based on regl-scatterplot'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b6534f19844762a18e6ab9124a46bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonyScatterplot(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL',…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_scatterplot import SymphonyScatterplot\n",
    "\n",
    "symph.widget(SymphonyScatterplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a918cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some datasets can contain duplicates: data instances that are the same or very similar to others. These can be hard to find, and become espeically problematic if the same data instance is in the training and testing splits. We can answer these questions using the Duplicates widget.\n",
    "\n",
    "Hint: Take a look at the `automobile` class, where there are duplicates across train and test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99eb410",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symphony spect dict value is {'width': 'XXL', 'height': 'M', 'page': 'Duplicates', 'name': 'SymphonyDuplicates', 'description': 'A Symphony component for inspecting potential duplicates in a dataset'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb823d2dbe745f98ea2aedbc518f2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonyDuplicates(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL', …"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_duplicates import SymphonyDuplicates\n",
    "\n",
    "symph.widget(SymphonyDuplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7579f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, we can use advanced ML metrics and the Familiarity widget to find the most and least representative data instances from a given datset, which can help identify model biases and annotation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "723aaa6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symphony spect dict value is {'width': 'XXL', 'height': 'M', 'page': 'Familiarity', 'name': 'SymphonyFamiliarity', 'description': 'A Symphony component to find outliers and common instances in a dataset'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b91bae01ad741a4abe4ede871b9c978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(SymphonyFamiliarity(layout=Layout(overflow='unset', width='100%'), widget_spec={'width': 'XXL',…"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symphony_familiarity import SymphonyFamiliarity\n",
    "\n",
    "symph.widget(SymphonyFamiliarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe645eb9",
   "metadata": {},
   "source": [
    "## Symphony as a Standalone Export\n",
    "\n",
    "Symphony can also be exported as a standalone static export to be shared with others or hosted. To explore this example in a web browser, you can export the report to local folder.\n",
    "\n",
    "If you only want to visualize locally without sharing the data, you can specify Symphony to handle the paths for a local standlone visualization by setting ``symlink_files`` to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5461b3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symphony spect dict value is {'filesPath': 'files/', 'dataType': 2, 'instancesPerPage': 40, 'showUnfilteredData': True, 'idColumn': 'id'}\n"
     ]
    }
   ],
   "source": [
    "symph.export('./symphony_report', name=\"Symphony CIFAR-10 Example\", symlink_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710755c2",
   "metadata": {},
   "source": [
    "You can now serve the dataset report. For example, from the `symphony_export` folder, run a simple server from the command line:\n",
    "\n",
    "```bash\n",
    "python -m http.server\n",
    "```\n",
    "\n",
    "And navigate to http://localhost:8000/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faff5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1638994625349,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "betterwithdata",
   "language": "python",
   "name": "betterwithdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
